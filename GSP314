// Deploy and Manage Cloud Environments with Google Cloud: Challenge Lab

Antern has the following resources that need to be migrated, copied, or recreated in Cymbal Directâ€™s existing cloud environment:

* A PostgreSQL database (running on a virtual machine) that needs to be migrated to Cloud SQL for PostgreSQL
* Containerized microservices application code to deploy on GKE (with reported reliability issues during testing that need to be troubleshooted)
* A VPC network with two subnetworks and firewalls that need to be created to connect new resources together
* IAM users across multiple projects that need to be granted the proper permissions and roles on specific resources

- Task 1: Migrate a stand-alone PostgreSQL database to a Cloud SQL for PostgreSQL instance
  In this task you must migrate the stand-alone PostgreSQL orders database running on the antern-postgresql-vm virtual machine to a
  Cloud SQL for PostgreSQL instance using a Database Migration Services continuous migration job and VPC Peering connectivity.

1.1 Prepare the stand-alone PostgreSQL database for migration. Note: For the first task, you will need to log in to the Antern Project with the Antern Owner credentials.

1.1.1 Enable the Google Cloud APIs required for Database Migration Services:
  Database Migration Services require the Database Migration API and the Service Networking API to be enabled in order to function. 
  You must enable these APIs for your project.
  
1.1.2 Upgrade the target databases on the antern-postgresql-vm virtual machine with the pglogical database extension.
      The pglogical database extension package that you must install is named postgresql-13-pglogical.
      To complete the configuration of the pglogical database extension you must edit the PostgreSQL configuration
      file /etc/postgresql/13/main/postgresql.conf to enable the pglogical database extension and you must edit 
      the /etc/postgresql/13/main/pg_hba.conf to allow access from all hosts.
      
      SSH on target DB's VM
      >sudo apt install postgresql-13-pglogical
      Enable plogical. En pg_hba.conf agrega:
      host    all all 0.0.0.0/0   md5
      >sudo nano /etc/postgresql/13/main/pg_hba.conf
      
      Y en postgresql.conf(pueden ir todosal final):
      >sudo nano /etc/postgresql/13/main/postgresql.conf
          #GSP918 - added configuration for pglogical database extension
          wal_level = logical         # minimal, replica, or logical
          max_worker_processes = 10   # one per database needed on provider node
                                      # one per node needed on subscriber node
          max_replication_slots = 10  # one per node needed on provider node
          max_wal_senders = 10        # one per node needed on provider node
          shared_preload_libraries = 'pglogical'
          max_wal_size = 1GB
          min_wal_size = 80MB
          listen_addresses = '*'         # what IP address(es) to listen on, '*' is all
    
      >sudo systemctl restart postgresql@13-main
      Launch psql
      >sudo -u postgres psql
      Add the pglogical database extension to orders db
      psql>\c orders;
      psql>CREATE EXTENSION pglogical;
      \c postgres;
      CREATE EXTENSION pglogical;

      
1.1.3 Create a dedicated user for database migration on the stand-alone database. (Replace migrtion_admin with other if needed)
  name: replication_admin
  pass:DMS_1s_cool!
  In psql:
  CREATE USER replication_admin PASSWORD 'DMS_1s_cool!';
  ALTER DATABASE orders OWNER TO replication_admin;
  ALTER ROLE replication_admin WITH REPLICATION;
  
1.1.4 Grant that user the required privileges and permissions for databases to be migrated. First check what tables exists
and then update the following
    psql>
  \c orders;
GRANT USAGE ON SCHEMA public TO replication_admin;
    GRANT ALL ON SCHEMA public TO replication_admin;
    GRANT SELECT ON public.distribution_centers TO replication_admin;
    GRANT SELECT ON public.inventory_items TO replication_admin;
    GRANT SELECT ON public.order_items TO replication_admin;
    GRANT SELECT ON public.products TO replication_admin;
    GRANT SELECT ON public.users TO replication_admin;

    GRANT USAGE ON SCHEMA pglogical TO replication_admin;
    GRANT ALL ON SCHEMA pglogical TO replication_admin;
    GRANT SELECT ON pglogical.tables TO replication_admin;
    GRANT SELECT ON pglogical.depend TO replication_admin;
    GRANT SELECT ON pglogical.local_node TO replication_admin;
    GRANT SELECT ON pglogical.local_sync_status TO replication_admin;
    GRANT SELECT ON pglogical.node TO replication_admin;
    GRANT SELECT ON pglogical.node_interface TO replication_admin;
    GRANT SELECT ON pglogical.queue TO replication_admin;
    GRANT SELECT ON pglogical.replication_set TO replication_admin;
    GRANT SELECT ON pglogical.replication_set_seq TO replication_admin;
    GRANT SELECT ON pglogical.replication_set_table TO replication_admin;
    GRANT SELECT ON pglogical.sequence_state TO replication_admin;
    GRANT SELECT ON pglogical.subscription TO replication_admin;

    ALTER TABLE public.distribution_centers OWNER TO replication_admin;
    ALTER TABLE public.inventory_items OWNER TO replication_admin;
    ALTER TABLE public.order_items OWNER TO replication_admin;
    ALTER TABLE public.products OWNER TO replication_admin;
    ALTER TABLE public.users OWNER TO replication_admin;
    ALTER TABLE public.inventory_items ADD PRIMARY KEY(id);

    
    \c postgres;
    GRANT USAGE ON SCHEMA pglogical TO replication_admin;
    GRANT ALL ON SCHEMA pglogical TO replication_admin;
    GRANT SELECT ON pglogical.tables TO replication_admin;
    GRANT SELECT ON pglogical.depend TO replication_admin;
    GRANT SELECT ON pglogical.local_node TO replication_admin;
    GRANT SELECT ON pglogical.local_sync_status TO replication_admin;
    GRANT SELECT ON pglogical.node TO replication_admin;
    GRANT SELECT ON pglogical.node_interface TO replication_admin;
    GRANT SELECT ON pglogical.queue TO replication_admin;
    GRANT SELECT ON pglogical.replication_set TO replication_admin;
    GRANT SELECT ON pglogical.replication_set_seq TO replication_admin;
    GRANT SELECT ON pglogical.replication_set_table TO replication_admin;
    GRANT SELECT ON pglogical.sequence_state TO replication_admin;
    GRANT SELECT ON pglogical.subscription TO replication_admin;



1.2 Migrate the stand-alone PostgreSQL database to a Cloud SQL for PostgreSQL instance

1.2.1 Create a new Database Migration Service connection profile for the stand-alone PostgreSQL database,
using the credentials of the Postgres Migration User migration user you created earlier.
You must configure the connection profile using the internal ip-address of the source compute instance.

  Username : import_user
  Password : DMS_1s_cool!
  
  In the Google Cloud Console, on the Navigation menu, click Database Migration > Connection profiles.

    Click + Create Profile.
    For Database engine, select PostgreSQL.
    For Connection profile name, enter XXXXX.
    For Hostname or IP address, enter the internal IP for the PostgreSQL source instance that you copied in the previous task
    (e.g., 10.128.0.2): 10.128.0.2
    For Port, enter 5432.
    For Username, enter migration_admin.
    For Password, enter DMS_1s_cool! .
    For Region select (region) .
    For all other values leave the defaults.
    Click Create.
    
 1.2.2 Create a new continuous Database Migration Service job.
 
 As part of the migration job configuration, make sure that you specify the following properties for the destination Cloud SQL instance:

  The Destination Instance ID must be set to Migrated Cloud SQL for PostgreSQL Instance ID
  The Password for the migrated instance must be set to supersecret!
  Database version must be set to Cloud SQL for PostgreSQL 13
  Region must be set to us-central1
  For Connections both Public IP and Private IP must be set
  Select a standard machine type with 1 vCPU
  For Storage type, use SSD
  Set the storage capacity to 10 GB
  For the Connectivity Method, you must use VPC peering with the default VPC network.

1.3 Promote a Cloud SQL to be a stand-alone instance for reading and writing data

  In the Google Cloud Console, on the Navigation menu, click Database Migration > Migration jobs.
  Click the migration job name "your_migration_job_name" to see the details page.
  Click Promote.
  If prompted to confirm, click Promote.
  When the promotion is complete, the status of the job will update to Completed.
  The migration job named "your_migration_job_name" has a status of complete.
  In the Google Cloud Console, on the Navigation menu, click Databases > SQL.
  Note that postgresql-cloudsql is now a stand-alone instance for reading and writing data.
  
- Task 2: Update permissions and add IAM roles to users:
  Now that the database has been migrated to a Cloud SQL for PostgreSQL instance, you will need to update user roles 
  via IAM for different members on the Antern and Cymbal teams. Specifically, you want to grant the Antern Editor 
  user access to the Cloud SQL database, the Cymbal Owner admin access to have full control of Cloud SQL resources, 
  and the Cymbal Editor to have editor permissions on the project. Note: For this task, you will need to log in to the 
  Antern Project with the Antern Owner credentials.
  
  2.1 Grant the Antern Editor user the Cloud SQL Instance User role for the CloudSQL database. 
      Their username is: Antern Editor username .

        Navigate to the Cloud SQL database you just created. In the Users section, add the Antern Editor user account
        to the database you created. Use Cloud IAM authentication and for the principal use their username above.
  2.2 Grant the Cymbal Owner user the Cloud SQL Admin role for the CloudSQL database.
      Their username is: Cymbal Owner username .

        Navigate to the Cloud SQL database you just created. In the Users section, add the Cymbal Owner user account
        to the database you created. Use Cloud IAM authentication and for the principal use their username above.
  2.3 Change the Cymbal Editor user role from Viewer to Editor. Their username is Cymbal Editor username.
  
- Task 3: Create networks and firewalls. Note: For this task, you will need to log in to the Cymbal Project 
          with the Cymbal Owner credentials.
          
3.1 Create a VPC network with two subnetworks
 
    3.1.1. Create a VPC network named network name with two subnets: subnet a name and subnet b name . Use a Regional dynamic routing mode.

    3.1.2. For subnet a name set the region to network region 1 .
      Set the IP stack type to IPv4 (single-stack)
      Set IPv4 range to 10.10.10.0/24
      
    3.1.3. For subnet b name set the region to network region 2 .
      Set the IP stack type to IPv4 (single-stack)
      Set IPv4 range to 10.10.20.0/24
      
3.2 Create firewall rules for the VPC network

    3.2.1 Create a firewall rule named firewall rule 1 .
      For the network, use network name .
      Set the priority to 65535, the traffic to Ingress and action to Allow
      The targets should be set to all instances in the network and the IP ranges should include all IPv4 ranges
      Set the Protocol to TCP and port to 22
      
    3.2.2 Create a firewall rule named firewall rule 2 .
      For the network, use network name .
      Set the priority to 65535, the traffic to Ingress and action to Allow
      The targets should be set to all instances in the network and the IP ranges should include all IPv4 ranges
      Set the Protocol to TCP and port to 3389
    3.2.3 Create a firewall rule named firewall rule 3 .
      For the network, use network name .
      Set the priority to 65535, the traffic to Ingress and action to Allow
      The targets should be set to all instances in the network and the IP ranges should include all IPv4 ranges
      Set the Protocol to icmp
      
- Task 4: Troubleshoot and fix a broken GKE cluster. Note: For this task, you will need to log in to the Cymbal Project
  with the Cymbal Owner credentials.
 update startup machine to update the cluster
      
